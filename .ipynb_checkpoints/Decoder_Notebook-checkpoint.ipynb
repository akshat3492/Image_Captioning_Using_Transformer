{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d0a0011-6c2d-4b60-b2b1-6575dea2bca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utility functions for data preprocessing and feature extraction and all other necessary libraries\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from Decoder.Decoder_Utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2ce11cf-c4be-4768-9f5a-61254bd4d46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess captions from the dataset and get vocabulary information\n",
    "\n",
    "df, word_to_idx, idx_to_word, vocab_size = preprocess_captions(\"./data/flickr30k_images/results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29a343eb-1d9f-4971-8c8c-4fab2bfa7ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "# Load a pre-trained ResNet18 model for feature extraction\n",
    "net = resnet18(num_classes=4)\n",
    "net.load_state_dict(torch.load(\"save_model/self_supervised_rotation_model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca10306a-d392-4139-9be1-b9cd2b46a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract image features using the ResNet18 model\n",
    "\n",
    "image_features = extract_image_features(net, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be0876cc-369a-4baf-94b3-d0a8ca7a3ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'image_features' (Tensor)\n"
     ]
    }
   ],
   "source": [
    "# %store image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2690c94-1ad8-4107-99fe-03daf0a01c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a002f6c7-c807-4b65-8313-bb23c3e2b746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Decoder.Decoder_DataSetup import ImageCaptionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aed7ab83-5798-48af-bfa2-b695d080a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame into training and validation sets\n",
    "df_train, df_val = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c10e8d01-022e-4946-84ab-5be4b78fc465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices for training and validation splits\n",
    "train_indices = df_train.index\n",
    "val_indices = df_val.index\n",
    "\n",
    "# Split img_features_tensor based on these indices\n",
    "img_features_train = image_features[train_indices]\n",
    "img_features_val = image_features[val_indices]\n",
    "\n",
    "# Create training and validation dataset instances\n",
    "train_dataset = ImageCaptionDataset(img_features_train, df_train)\n",
    "val_dataset = ImageCaptionDataset(img_features_val, df_val)\n",
    "\n",
    "#batch size\n",
    "batch_size = 128\n",
    "\n",
    "# data loaders\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eba15999-d0ea-4723-9f5c-2dfb12b663db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27111005-e87d-4487-8e57-77cc26cbbbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Decoder.transformer import Transformer\n",
    "import torch.optim as optim\n",
    "model = Transformer(\n",
    "    word_to_idx=word_to_idx, \n",
    "    input_dim=512,  \n",
    "    wordvec_dim=128,  \n",
    "    num_heads=4, \n",
    "    num_layers=6,\n",
    "    max_length=17\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ce79f6f-35a7-40f1-81aa-84ea6c3b0c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import training function and learning rate scheduler\n",
    "\n",
    "from Decoder.Decoder_Train import train_model\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_idx[\"<TAB>\"])  # Cross-entropy loss, ignoring padding index\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Adam optimizer with weight decay\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True # Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2de2da57-5d68-4f40-b235-2a835e7b7dbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 5.1270\n",
      "Validation Loss: 4.4356\n",
      "Epoch [2/5], Loss: 4.2864\n",
      "Validation Loss: 4.0439\n",
      "Epoch [3/5], Loss: 4.0414\n",
      "Validation Loss: 3.8444\n",
      "Epoch [4/5], Loss: 3.9126\n",
      "Validation Loss: 3.7625\n",
      "Epoch [5/5], Loss: 3.8209\n",
      "Validation Loss: 3.6798\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_data_loader, val_data_loader, optimizer, scheduler, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c01407-4722-4c17-8897-de7d012fa8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a346b21b-d569-4ecf-b28e-eed0e2ffbcd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
